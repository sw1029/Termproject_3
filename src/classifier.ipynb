{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf7d92edb9231de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c8e2527900435d896b7c9258a9ace5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "투표 결과: Counter({3: 5}), 최종 선택: 3\n",
      "입력: 오늘 학식 뭐 나와?\n",
      "예측: 3 (식단 안내)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import difflib\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# 1. 모델 및 토크나이저 로드 (기존과 동일)\n",
    "model_name = \"Qwen/Qwen3-14B\"\n",
    "# model_name = \"dnotitia/DNA-R1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 'categories' 딕셔너리가 함수 외부 또는 전역 스코프에 정의되어 있어야 합니다.\n",
    "categories = {\n",
    "    0: \"졸업요건\",\n",
    "    1: \"학교 공지사항\",\n",
    "    2: \"학사일정\",\n",
    "    3: \"식단 안내\",\n",
    "    4: \"통학/셔틀 버스\"\n",
    "}\n",
    "\n",
    "def _get_single_llm_prediction(user_query):\n",
    "    # 1. 프롬프트를 'system'과 'user' 역할로 분리합니다.\n",
    "    system_prompt = \"당신은 주어진 학교 관련 질문을 사전 정의된 5개의 카테고리 중 하나로 분류하는 AI 분류기입니다. 오직 숫자 레이블 하나만 응답해야 합니다.\"\n",
    "\n",
    "    user_prompt = f\"\"\"다음은 사용자 질문과 관련된 카테고리 목록입니다:\n",
    "0: 졸업요건 (예: 졸업까지 몇 학점을 들어야 하나요?)\n",
    "1: 학교 공지사항 (예: 이번에 올라온 공지사항 어디서 볼 수 있어요?)\n",
    "2: 학사일정 (예: 이번 학기 수강신청은 언제 시작하나요?)\n",
    "3: 식단 안내 (예: 오늘 학식 뭐 나와요?)\n",
    "4: 통학/셔틀 버스 (예: 다음주에 셔틀버스는 정상 운행하나요?)\n",
    "\n",
    "사용자 질문: \"{user_query}\"\n",
    "\n",
    "위 질문은 어떤 카테고리에 가장 적합한가요? 숫자 레이블만 대답해주세요 (0, 1, 2, 3, 4 중 하나).\n",
    "\n",
    "가장 적합한 카테고리 번호:\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    # 2. Qwen2의 채팅 템플릿을 적용하여 모델이 이해하는 형식으로 변환합니다.\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    # 3. 변환된 텍스트를 토큰화합니다.\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Qwen2 토크나이저에 pad_token이 없을 수 있으므로 eos_token으로 설정해주는 것이 안전합니다.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 4. 모델을 통해 답변을 생성합니다. (기존 generate 파라미터 유지)\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=5,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # 5. 입력 부분을 제외하고 순수하게 생성된 부분만 디코딩합니다.\n",
    "    # 제공해주신 Qwen2 예제 코드의 디코딩 방식을 적용합니다.\n",
    "    input_ids_len = model_inputs.input_ids.shape[1]\n",
    "    response_ids = generated_ids[:, input_ids_len:]\n",
    "\n",
    "    response_text = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    #print(f\"모델에서 추출한 response_text(label 파싱 전): {response_text}\")\n",
    "    \n",
    "    # 6. 응답에서 숫자 레이블을 파싱합니다. (기존 로직과 동일)\n",
    "    try:\n",
    "        predicted_label_str = ''.join(filter(str.isdigit, response_text))\n",
    "        if predicted_label_str:\n",
    "            predicted_label = int(predicted_label_str[0])\n",
    "            if predicted_label in categories:\n",
    "                return predicted_label\n",
    "    except (ValueError, IndexError):\n",
    "        # 파싱에 실패한 경우\n",
    "        return None\n",
    "\n",
    "    # 유효한 카테고리 번호가 아닌 경우\n",
    "    return None\n",
    "\n",
    "\n",
    "def _fallback_classify_by_similarity(user_query):\n",
    "    \"\"\"\n",
    "    LLM 예측 실패 시, 키워드 유사도 기반으로 분류하는 Fallback 함수입니다.\n",
    "    \"\"\"\n",
    "    category_examples = {\n",
    "        0: \"졸업 요건, 졸업 학점, 졸업 논문, 졸업 자격, 졸업 인증, 유예 신청, 졸업 필수 조건\",\n",
    "        1: \"학교 공지사항, 안내문, 공고, 공지 업데이트, 휴강 안내, 장학금 공지, 긴급 알림\",\n",
    "        2: \"수강 신청, 시험 기간, 성적 발표, 개강일, 종강일, 학사 일정, 수업 일정, 등록 일정\",\n",
    "        3: \"학식, 학생 식당, 식단표, 조식, 중식, 석식, 오늘 메뉴, 급식 시간, 식단 운영\",\n",
    "        4: \"셔틀버스, 통학버스, 운행 시간표, 버스 노선, 정류장 위치, 셔틀 예약, 통학 교통\"\n",
    "    }\n",
    "\n",
    "    best_score = -1\n",
    "    best_label = random.randint(0, 4) # 만약을 대비한 기본값\n",
    "    for label, example in category_examples.items():\n",
    "        score = difflib.SequenceMatcher(None, user_query, example).ratio()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_label = label\n",
    "\n",
    "    return best_label\n",
    "\n",
    "\n",
    "def classify_query(user_query, num_votes=5):\n",
    "    \"\"\"\n",
    "    사용자 질의를 입력받아 Voting 앙상블 기법으로 카테고리를 분류합니다.\n",
    "\n",
    "    Args:\n",
    "        user_query (str): 사용자 질문\n",
    "        num_votes (int): 앙상블에 사용할 투표 횟수 (홀수를 추천)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (예측된 레이블, 예측된 카테고리 이름)\n",
    "    \"\"\"\n",
    "    votes = []\n",
    "    # 1. 정해진 횟수만큼 LLM에게 예측을 요청하여 투표 수집\n",
    "    for _ in range(num_votes):\n",
    "        prediction = _get_single_llm_prediction(user_query)\n",
    "        #print(f\"get single llm prediction의 결과물: {prediction}\")\n",
    "        if prediction is not None:\n",
    "            votes.append(prediction)\n",
    "\n",
    "    # 2. 투표 결과 분석\n",
    "    if votes:\n",
    "        # 가장 많이 나온 투표 결과를 선택 (다수결)\n",
    "        counter = Counter(votes)\n",
    "        # most_common(1)은 [(가장 흔한 항목, 횟수)] 형태의 리스트를 반환\n",
    "        best_label = counter.most_common(1)[0][0]\n",
    "        print(f\"투표 결과: {counter}, 최종 선택: {best_label}\")\n",
    "    else:\n",
    "        # 3. LLM이 모든 예측에 실패한 경우, Fallback 로직 실행\n",
    "        print(\"LLM 예측이 모두 실패하여 유사도 기반 Fallback 로직을 실행합니다.\")\n",
    "        best_label = _fallback_classify_by_similarity(user_query)\n",
    "\n",
    "    return best_label, categories[best_label]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"오늘 학식 뭐 나와?\"\n",
    "    label, category = classify_query(q)\n",
    "    print(f\"입력: {q}\\n예측: {label} ({category})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bb2d1f-a93f-4165-81fe-0bb45123aaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "모델에서 추출한 response_text(label 파싱 전): 3\n",
      "get single llm prediction의 결과물: 3\n",
      "투표 결과: Counter({3: 5}), 최종 선택: 3\n",
      "2025-06-20 07:51:36.074 | INFO     | src.answers.meals_answer:get_context:184 - '%s' 날짜의 로컬 식단 정보를 사용합니다.\n",
      "찾은 식단 정보: 메뉴운영내역, 운영안함, 운영안함 등\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = str(Path.cwd().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "from src.answers import academic_calendar_answer,shuttle_bus_answer,graduation_req_answer,meals_answer,notices_answer\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "import torch\n",
    "from threading import Thread\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, BitsAndBytesConfig\n",
    "from utils.config import settings\n",
    "\n",
    "\n",
    "def format_meals_context(docs: List[Dict[str, Any]]) -> List[str]:\n",
    "    formatted_texts = []\n",
    "    for doc in docs:\n",
    "        meal = doc.get(\"meal\", \"\")\n",
    "        menu = doc.get(\"menu\", \"운영안함\").replace(\"\\n\", \", \")\n",
    "        if menu != \"운영안함\":\n",
    "            formatted_texts.append(f\"[{meal}] {menu}\")\n",
    "    return formatted_texts\n",
    "\n",
    "\n",
    "class HybridRetriever:\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        return []\n",
    "\n",
    "\n",
    "class PromptBuilder:\n",
    "    def build(self, question: str, docs: List[str]) -> str:\n",
    "        context = \"\".join(docs) if docs else \"참고할 정보가 없습니다.\"\n",
    "        prompt = f\"\"\"당신은 충남대학교 관련 정보를 안내하는 챗봇입니다.\n",
    "주어진 '참고 자료'를 근거로 사용자의 질문에 명확하고 친절하게 답변해야 합니다.\n",
    "참고 자료에 없는 내용은 답변에 포함하지 마세요.\n",
    "\n",
    "[참고 자료]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변]\n",
    "\"\"\"\n",
    "        return prompt.strip()\n",
    "\n",
    "\n",
    "class AnswerGenerator:\n",
    "    def __init__(self) -> None:\n",
    "        self.model_type = settings.generator_model_type\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.client = None\n",
    "        self.streamer = None\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        if self.model_type == \"local\":\n",
    "            model_name = settings.generator_model_name_or_path\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                use_safetensors=True,\n",
    "                quantization_config=bnb_config,\n",
    "            )\n",
    "            self.streamer = TextIteratorStreamer(\n",
    "                self.tokenizer, skip_prompt=True, skip_special_tokens=True\n",
    "            )\n",
    "            print(f\"✅ Local Generator Model Loaded: {model_name}\")\n",
    "\n",
    "    def generate(self, question: str, docs: List[Dict[str, Any]], max_new_tokens: int = 512, temperature: float = 0.0):\n",
    "        if docs and isinstance(docs[0], dict) and \"menu\" in docs[0]:\n",
    "            doc_texts = format_meals_context(docs)\n",
    "        else:\n",
    "            doc_texts = [json.dumps(d, ensure_ascii=False) for d in docs]\n",
    "        prompt = PromptBuilder().build(question, doc_texts)\n",
    "\n",
    "        if self.model_type == \"local\" and self.model and self.tokenizer:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            generation_kwargs = dict(\n",
    "                input_ids=inputs.input_ids,\n",
    "                streamer=self.streamer,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=temperature > 0.0,\n",
    "                temperature=temperature if temperature > 0.0 else None,\n",
    "            )\n",
    "            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n",
    "            thread.start()\n",
    "            for token in self.streamer:\n",
    "                yield token\n",
    "            return\n",
    "        if self.model_type == \"openai\" and self.client:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=settings.openai_model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"당신은 충남대학교 정보를 안내하는 챗봇입니다.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "            )\n",
    "            yield response.choices[0].message.content.strip()\n",
    "            return\n",
    "        yield \"답변 생성 모델이 올바르게 설정되지 않았습니다.\"\n",
    "\n",
    "\n",
    "ANSWER_HANDLERS = {\n",
    "    0: graduation_req_answer.generate_answer,\n",
    "    1: notices_answer.generate_answer,\n",
    "    2: academic_calendar_answer.generate_answer,\n",
    "    3: meals_answer.generate_answer,\n",
    "    4: shuttle_bus_answer.generate_answer,\n",
    "}\n",
    "\n",
    "def generate_response(question: str) -> str:\n",
    "    label, _ = classify_query(question)\n",
    "    handler = ANSWER_HANDLERS.get(label)\n",
    "    if handler:\n",
    "        return handler(question)\n",
    "    retriever = HybridRetriever()\n",
    "    docs = retriever.retrieve(question)\n",
    "    generator = AnswerGenerator()\n",
    "    return ''.join(generator.generate(question, docs))\n",
    "\n",
    "sample_question = '오늘 학식 뭐 나와?'\n",
    "print(generate_response(sample_question))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e421ce51-b4e7-44a1-8dfb-530191149afa",
   "metadata": {
    "id": "e421ce51-b4e7-44a1-8dfb-530191149afa",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomized_korean_questions_result.json:   0%|                                      | 0/104 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "투표 결과: Counter({4: 5}), 최종 선택: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n",
      "모델에서 추출한 response_text(label 파싱 전): 4\n",
      "get single llm prediction의 결과물: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "randomized_korean_questions_result.json:   0%|                                      | 0/104 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m question \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m label, _ \u001b[38;5;241m=\u001b[39m classify_query(question)  \u001b[38;5;66;03m# VARCO 모델 사용\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델이 생성한 대답\u001b[39;00m\n\u001b[1;32m     19\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label})\n\u001b[1;32m     20\u001b[0m qa_results\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m: question, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m: answer})\n",
      "Cell \u001b[0;32mIn[3], line 120\u001b[0m, in \u001b[0;36mgenerate_response\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_response\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 120\u001b[0m     label, _ \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     handler \u001b[38;5;241m=\u001b[39m ANSWER_HANDLERS\u001b[38;5;241m.\u001b[39mget(label)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handler:\n",
      "Cell \u001b[0;32mIn[2], line 143\u001b[0m, in \u001b[0;36mclassify_query\u001b[0;34m(user_query, num_votes)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# 1. 정해진 횟수만큼 LLM에게 예측을 요청하여 투표 수집\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_votes):\n\u001b[0;32m--> 143\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43m_get_single_llm_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget single llm prediction의 결과물: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m, in \u001b[0;36m_get_single_llm_prediction\u001b[0;34m(user_query)\u001b[0m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# 4. 모델을 통해 답변을 생성합니다. (기존 generate 파라미터 유지)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# 5. 입력 부분을 제외하고 순수하게 생성된 부분만 디코딩합니다.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 제공해주신 Qwen2 예제 코드의 디코딩 방식을 적용합니다.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m input_ids_len \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/term/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/term/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/term/lib/python3.10/site-packages/transformers/generation/utils.py:3548\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3546\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3548\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3549\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3550\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3552\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/term/lib/python3.10/site-packages/transformers/generation/utils.py:2748\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[1;32m   2746\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2747\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2748\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# data/question directory relative to this notebook\n",
    "base_dir = Path('../data/question') if Path('../data/question').exists() else Path('data/question')\n",
    "output_dir = Path('../outputs') if Path('../outputs').exists() else Path('outputs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for path in base_dir.glob('*.json'):\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        questions_data = json.load(f)\n",
    "    results = []  # question-label pairs\n",
    "    qa_results = []  # question-answer pairs\n",
    "    for item in tqdm(questions_data, desc=path.name):\n",
    "        question = item['question']\n",
    "        label, _ = classify_query(question)  # VARCO 모델 사용\n",
    "        answer = generate_response(question)  # 모델이 생성한 대답\n",
    "        results.append({'question': question, 'label': label})\n",
    "        qa_results.append({'question': question, 'answer': answer})\n",
    "    out_file = output_dir / f\"{path.stem}_output.json\"\n",
    "    qa_file = output_dir / f\"{path.stem}_answer_output.json\"\n",
    "    with out_file.open('w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    with qa_file.open('w', encoding='utf-8') as f:\n",
    "        json.dump(qa_results, f, ensure_ascii=False, indent=2)\n",
    "    print(f'✅ {out_file} 저장 완료')\n",
    "    print(f'✅ {qa_file} 저장 완료')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84323d-ee2f-4f72-9239-58ee5ce85aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. 경로 설정 ---\n",
    "# 웹 UI가 질문을 저장하는 폴더 (입력)\n",
    "QUESTION_DIR = Path('../question')\n",
    "\n",
    "# 분류된 답변을 저장하는 폴더 (출력)\n",
    "ANSWER_DIR = Path('../answer')\n",
    "\n",
    "# 처리가 완료된 질문을 옮길 폴더\n",
    "PROCESSED_DIR = QUESTION_DIR / 'processed'\n",
    "\n",
    "# 오류 발생 시 질문을 옮길 폴더\n",
    "ERROR_DIR = QUESTION_DIR / 'error'\n",
    "\n",
    "# 폴더가 없는 경우 생성\n",
    "QUESTION_DIR.mkdir(exist_ok=True)\n",
    "ANSWER_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "ERROR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "print(f\"'{QUESTION_DIR}' 폴더를 실시간으로 감지합니다...\")\n",
    "print(\"노트북 셀을 중단(Interrupt)하면 감지가 종료됩니다.\")\n",
    "\n",
    "# --- 2. 실시간 감지 및 처리 루프 ---\n",
    "try:\n",
    "    while True:\n",
    "        # question 폴더에서 아직 처리되지 않은 질문 파일 목록을 가져옴\n",
    "        # 파일 이름이 'q_'로 시작하는 json 파일만 대상으로 함\n",
    "        question_files = list(QUESTION_DIR.glob('q_*.json'))\n",
    "\n",
    "        # 처리할 파일이 없으면 5초 대기 후 다시 확인\n",
    "        if not question_files:\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "        # 새로운 질문 파일을 하나씩 처리\n",
    "        for q_path in question_files:\n",
    "            try:\n",
    "                # 1. 질문 파일 읽기\n",
    "                with q_path.open('r', encoding='utf-8') as f:\n",
    "                    q_data = json.load(f)\n",
    "                \n",
    "                question_text = q_data.get('text', '')\n",
    "                question_id = q_data.get('question_id', q_path.stem)\n",
    "\n",
    "                # 2. 질문 분류 (기존에 정의된 classify_query 함수 사용)\n",
    "                label, label_text = classify_query(question_text)\n",
    "\n",
    "                # 3. 답변 파일 생성\n",
    "                # 웹 UI가 질문(q_id)에 맞는 답변(a_id)을 찾을 수 있도록 파일 이름을 맞춤\n",
    "                answer_id = question_id.replace('q_', 'a_')\n",
    "                answer_path = ANSWER_DIR / f\"{answer_id}.json\"\n",
    "                \n",
    "                answer_data = {\n",
    "                    'question_id': question_id,\n",
    "                    'original_question': question_text,\n",
    "                    'label': label,\n",
    "                    'label_text': label_text,\n",
    "                    'classified_at': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "                # 답변 파일을 JSON 형식으로 저장\n",
    "                with answer_path.open('w', encoding='utf-8') as f:\n",
    "                    json.dump(answer_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "                # 4. 처리 완료된 질문 파일을 processed 폴더로 이동\n",
    "                shutil.move(str(q_path), PROCESSED_DIR / q_path.name)\n",
    "                \n",
    "                # 5. 노트북에 처리 로그 출력\n",
    "                clear_output(wait=True) # 이전 출력 지우기\n",
    "                print(f\"'{QUESTION_DIR}' 폴더를 실시간으로 감지합니다...\")\n",
    "                print(\"노트북 셀을 중단(Interrupt)하면 감지가 종료됩니다.\\n\")\n",
    "                print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] 처리 완료:\")\n",
    "                print(f\"  - 질문 ID: {question_id}\")\n",
    "                print(f\"  - 내   용: \\\"{question_text}\\\"\")\n",
    "                print(f\"  - 분   류: {label} ({label_text})\")\n",
    "                print(f\"  - 답변 파일: {answer_path}\\n\")\n",
    "                print(\"다음 질문을 기다립니다...\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"파일 처리 중 오류 발생: {q_path.name}, 오류: {e}\")\n",
    "                # 오류 발생 시 해당 파일을 error 폴더로 이동\n",
    "                shutil.move(str(q_path), ERROR_DIR / q_path.name)\n",
    "\n",
    "# 사용자가 직접 셀 실행을 중단할 경우 (KeyboardInterrupt) 루프 종료\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n실시간 분류 프로세스를 중단합니다.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
